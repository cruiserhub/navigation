{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#quick-start\" data-toc-modified-id=\"quick-start-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>quick start</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Resources</a></span></li></ul></li><li><span><a href=\"#FULL-MODEL\" data-toc-modified-id=\"FULL-MODEL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>FULL MODEL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Qnetwork\" data-toc-modified-id=\"Qnetwork-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Qnetwork</a></span></li></ul></li><li><span><a href=\"#POMDP-RNN-Game\" data-toc-modified-id=\"POMDP-RNN-Game-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>POMDP RNN Game</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-setting：-grid-=--(3,7)，-holes-=-0\" data-toc-modified-id=\"Standard-setting：-grid-=--(3,7)，-holes-=-0-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Standard setting： grid =  (3,7)， holes = 0</a></span></li><li><span><a href=\"#Model-Tranining\" data-toc-modified-id=\"Model-Tranining-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Model Tranining</a></span></li><li><span><a href=\"#decoding-vs-performance\" data-toc-modified-id=\"decoding-vs-performance-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>decoding vs performance</a></span></li><li><span><a href=\"#learning-rate-vs-performance\" data-toc-modified-id=\"learning-rate-vs-performance-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>learning rate vs performance</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Anlytic-part-,-check-the-behaviour-correspond-to-each-decoding-level-and-explain-why\" data-toc-modified-id=\"Anlytic-part-,-check-the-behaviour-correspond-to-each-decoding-level-and-explain-why-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span>Anlytic part , check the behaviour correspond to each decoding level and explain why</a></span></li></ul></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>PCA</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test what is the decoding change for single size training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [*The* Reinforcement learning book from Sutton & Barto](http://incompleteideas.net/sutton/book/the-book-2nd.html)\n",
    "* [The REINFORCE paper from Ronald J. Williams (1992)](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "# write a new algorithm with a pointer or attention on top of th two\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import POMDPgame_bars\n",
    "from POMDPgame_bars import*\n",
    "\n",
    "import POMDPgame_basic\n",
    "from POMDPgame_basic import*\n",
    "\n",
    "import POMDPgame_holes\n",
    "from POMDPgame_holes import*\n",
    "\n",
    "\n",
    "import RNN\n",
    "from RNN import *\n",
    "\n",
    "import navigation2\n",
    "from navigation2 import*\n",
    "\n",
    "import Nets\n",
    "from Nets import*\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qnetwork\n",
    "\n",
    "To select actions we take maximum of Q value, corresponding to certain move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the liquid state approach to work, you need a lot of neurons as surplus or enough hidden to hidden connectivity to make it have an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  POMDP RNN Game\n",
    "\n",
    "In this game , we use a new reward function determined by game, if the agent achieves the goal before 50, reward is 1. If time pass 50 reward is 0.5, once time pass 100 agent gets a reward of -0.5 .  Practically, this is found to be easier to learn than the rewards as a continous function of time.  Tf the agent learns to search in a efficient way, the largest possible way for search is to firstly arrive at corner then goes to the goal, which, takes about 50 steps, it is reasonble to make 50 and 100 as milestone thing.  Also in principe as the game doesn't have a timer , it is not if it can use a reward as funtion of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 condition for ending , when pass time limit, game over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For weight update, it seems to be better do it after episode, as it makes non-sense evaluate strategy during episode, but a the end. Also, it is much quicker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming of MDP here, hidden state is as state of enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of how to make this traning stable, adding exploration noise , intenral noise or environmental variability like multiple mazes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task <class 'POMDPgame_scale.GameScale'>\n",
      "scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yons/.local/lib/python3.6/site-packages/torch/tensor.py:293: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance 0.6062186131026897\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance -0.19379810723870125\n"
     ]
    }
   ],
   "source": [
    "# moving bar train\n",
    "trial = 399\n",
    "tasks = ['scale']\n",
    "iterations = [4]\n",
    "# iterations = [1, 1, 1, 1, 1, 1, 1]\n",
    "Scores = np.zeros((1, 5))\n",
    "i = 0\n",
    "# weight_read = 'weights_' + 'scale' + '/rnn_1515tanh512_checkpoint{}_{}'.format(399, 5)\n",
    "for iters, task in zip(iterations, tasks):\n",
    "        Task = MultipleTasks(task=task, weight_write='weights_cpu/rnn_1515tanh512_checkpoint{}'.format(0) \\\n",
    "                         , noise=0.0, weight1='weights_cpu_pos/rnn_1515tanh512_checkpoint399',\n",
    "                         weight2='weights_cpu_mem/rnn_1515tanh512_checkpoint49')\n",
    "        for k in range(2):\n",
    "            weight_read = 'weights_' + task + '/rnn_1515tanh512_checkpoint{}_{}'.format(trial, 6 - k)\n",
    "            if task == 'scale':\n",
    "                score = Test(task, Task.game, weight = weight_read, size = 50, test_size = 2, limit_set = 4, step = 3)\n",
    "            elif (task == 'scale_x') or (task == 'scale_y'):\n",
    "                score = Test(task, Task.game, weight = weight_read, test_size = 1, limit_set = 4)\n",
    "            else:\n",
    "                score = Test(task, Task.game, weight = weight_read, limit_set = 2)\n",
    "            Scores[i, k]= score\n",
    "        i += 1\n",
    "np.save('Scores_scale', Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task <class 'POMDPgame_scale_y.GameScale_y'>\n",
      "scale_y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yons/.local/lib/python3.6/site-packages/torch/tensor.py:293: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_y performance 0.3649136089421072\n",
      "scale_y performance 0.02388265837024215\n",
      "scale_y performance 0.8186569467317381\n"
     ]
    }
   ],
   "source": [
    "# moving bar train\n",
    "trial = 399\n",
    "tasks = ['scale_y']\n",
    "iterations = [4]\n",
    "# iterations = [1, 1, 1, 1, 1, 1, 1]\n",
    "Scores = np.zeros((1, 5))\n",
    "i = 0\n",
    "# weight_read = 'weights_' + 'scale' + '/rnn_1515tanh512_checkpoint{}_{}'.format(399, 5)\n",
    "for iters, task in zip(iterations, tasks):\n",
    "        Task = MultipleTasks(task=task, weight_write='weights_cpu/rnn_1515tanh512_checkpoint{}'.format(0) \\\n",
    "                         , noise=0.0, weight1='weights_cpu_pos/rnn_1515tanh512_checkpoint399',\n",
    "                         weight2='weights_cpu_mem/rnn_1515tanh512_checkpoint49')\n",
    "        for k in range(5):\n",
    "            weight_read = 'weights_' + task + '/rnn_1515tanh512_checkpoint{}_{}'.format(trial, 9 - k)\n",
    "            if task == 'scale':\n",
    "                score = Test(task, Task.game, weight = weight_read, size = 50, test_size = 2, limit_set = 4, step = 3)\n",
    "            elif (task == 'scale_x') or (task == 'scale_y'):\n",
    "                score = Test(task, Task.game, weight = weight_read, test_size = 1, limit_set = 4)\n",
    "            else:\n",
    "                score = Test(task, Task.game, weight = weight_read, limit_set = 2)\n",
    "            Scores[i, k]= score\n",
    "        i += 1\n",
    "np.save('Scores_scale', Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task <class 'POMDPgame_basic.GameBasic'>\n",
      "basic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yons/.local/lib/python3.6/site-packages/torch/tensor.py:293: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic performance 0.9031684078176023\n",
      "basic performance 0.9292655760835254\n",
      "basic performance 0.9204456103268506\n",
      "basic performance 0.9565360776899936\n",
      "basic performance 0.9438141953017409\n",
      "task <class 'POMDPgame_holes.GameHole'>\n",
      "hole\n",
      "hole performance 0.6746172556586527\n",
      "hole performance 0.5689470329838906\n",
      "hole performance 0.6639240090923737\n",
      "hole performance 0.6890389812398598\n",
      "hole performance 0.7242161812663229\n",
      "task <class 'POMDPgame_bars.GameBar'>\n",
      "bar\n",
      "bar performance 0.6801895145466739\n",
      "bar performance 0.6627369944749971\n",
      "bar performance 0.6871498169591541\n",
      "bar performance 0.6992439398019161\n",
      "bar performance 0.647436320141778\n",
      "task <class 'POMDPgame_scale.GameScale'>\n",
      "scale\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance -0.7352362239514121\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance 0.32732565036318806\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance -0.06033695633960101\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance 0.5983250657577792\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance -0.22192771082991034\n",
      "task <class 'POMDPgame_scale_x.GameScale_x'>\n",
      "scale_x\n",
      "scale_x performance 0.8455349813171358\n",
      "scale_x performance 0.8866606060216741\n",
      "scale_x performance 0.9194871428638097\n",
      "scale_x performance 0.9041518819004601\n",
      "scale_x performance 0.8657161644912175\n",
      "task <class 'POMDPgame_scale_y.GameScale_y'>\n",
      "scale_y\n",
      "scale_y performance -0.12372207129700317\n",
      "scale_y performance 0.2167265763821949\n",
      "scale_y performance 0.5607116496334521\n",
      "scale_y performance 0.10759516852580585\n",
      "scale_y performance 0.3503846882407393\n",
      "task <class 'POMDPgame_implicit.GameImplicit'>\n",
      "implicit\n",
      "implicit performance 0.697861919203209\n",
      "implicit performance 0.6054311581552871\n",
      "implicit performance 0.6971980227854782\n",
      "implicit performance 0.8254533230469263\n",
      "implicit performance 0.6258299304535797\n"
     ]
    }
   ],
   "source": [
    "#### moving bar train\n",
    "trial = 399\n",
    "tasks = ['basic', 'hole', 'bar', 'scale', 'scale_x', 'scale_y', 'implicit']\n",
    "iterations = [9, 29, 29, 9, 29, 29, 29]\n",
    "# iterations = [1, 1, 1, 1, 1, 1, 1]\n",
    "Scores = np.zeros((7, 5))\n",
    "i = 0\n",
    "# weight_read = 'weights_' + 'scale' + '/rnn_1515tanh512_checkpoint{}_{}'.format(399, 5)\n",
    "for iters, task in zip(iterations, tasks):\n",
    "        Task = MultipleTasks(task=task, weight_write='weights_cpu/rnn_1515tanh512_checkpoint{}'.format(0) \\\n",
    "                         , noise=0.0, weight1='weights_cpu_pos/rnn_1515tanh512_checkpoint399',\n",
    "                         weight2='weights_cpu_mem/rnn_1515tanh512_checkpoint49')\n",
    "        for k in range(5):\n",
    "            weight_read = 'weights_' + task + '/rnn_1515tanh512_checkpoint{}_{}'.format(trial,  iters - k)\n",
    "            if task == 'scale':\n",
    "                score = Test(task, Task.game, weight = weight_read, size = 50, test_size = 2, limit_set = 4, step = 3)\n",
    "            elif (task == 'scale_x') or (task == 'scale_y'):\n",
    "                score = Test(task, Task.game, weight = weight_read, test_size = 1, limit_set = 4)\n",
    "            else:\n",
    "                score = Test(task, Task.game, weight = weight_read)\n",
    "            Scores[i, k]= score\n",
    "        i += 1\n",
    "np.save('Scores_modules', Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task <class 'POMDPgame_basic.GameBasic'>\n",
      "basic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yons/.local/lib/python3.6/site-packages/torch/tensor.py:293: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic performance 0.9339615118069065\n",
      "basic performance 0.9304340718755142\n",
      "basic performance 0.9532086957925652\n",
      "basic performance 0.9628908475956623\n",
      "basic performance 0.9378663016288779\n",
      "task <class 'POMDPgame_holes.GameHole'>\n",
      "hole\n",
      "hole performance 0.7402882456923582\n",
      "hole performance 0.6914422832109195\n",
      "hole performance 0.6996684938740919\n",
      "hole performance 0.7073232140178183\n",
      "hole performance 0.7583449039148773\n",
      "task <class 'POMDPgame_bars.GameBar'>\n",
      "bar\n",
      "bar performance 0.7525221616876329\n",
      "bar performance 0.7457533812179759\n",
      "bar performance 0.7209314194122264\n",
      "bar performance 0.7555190514046531\n",
      "bar performance 0.7228026752236519\n",
      "task <class 'POMDPgame_scale.GameScale'>\n",
      "scale\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance -0.7246805346780507\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance 0.313059165607429\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance -0.06168218106440998\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance 0.6180081347017202\n",
      "(27, 14)\n",
      "(27, 39)\n",
      "scale performance -0.21070521449411628\n",
      "task <class 'POMDPgame_scale_x.GameScale_x'>\n",
      "scale_x\n",
      "scale_x performance 0.8130994595539218\n",
      "scale_x performance 0.8919484604550694\n",
      "scale_x performance 0.9207742825497285\n",
      "scale_x performance 0.8920610718151996\n",
      "scale_x performance 0.8556792663468864\n",
      "task <class 'POMDPgame_scale_y.GameScale_y'>\n",
      "scale_y\n",
      "scale_y performance -0.12132067770713151\n",
      "scale_y performance 0.23638107866082533\n",
      "scale_y performance 0.5897382876466166\n",
      "scale_y performance 0.12000348567037288\n",
      "scale_y performance 0.33747316363587276\n",
      "task <class 'POMDPgame_implicit.GameImplicit'>\n",
      "implicit\n",
      "implicit performance 0.763844742696539\n",
      "implicit performance 0.7180150490684472\n",
      "implicit performance 0.7991764140143729\n",
      "implicit performance 0.8558015776623282\n",
      "implicit performance 0.7475211441161211\n"
     ]
    }
   ],
   "source": [
    "#### moving bar train\n",
    "trial = 399\n",
    "tasks = ['basic', 'hole', 'bar', 'scale', 'scale_x', 'scale_y', 'implicit']\n",
    "iterations = [9, 29, 29, 9, 29, 29, 29]\n",
    "# iterations = [1, 1, 1, 1, 1, 1, 1]\n",
    "Scores = np.zeros((7, 5))\n",
    "i = 0\n",
    "# weight_read = 'weights_' + 'scale' + '/rnn_1515tanh512_checkpoint{}_{}'.format(399, 5)\n",
    "for iters, task in zip(iterations, tasks):\n",
    "        Task = MultipleTasks(task=task, weight_write='weights_cpu/rnn_1515tanh512_checkpoint{}'.format(0) \\\n",
    "                         , noise=0.0, weight1='weights_cpu_pos/rnn_1515tanh512_checkpoint399',\n",
    "                         weight2='weights_cpu_mem/rnn_1515tanh512_checkpoint49')\n",
    "        for k in range(5):\n",
    "            weight_read = 'weights_' + task + '/rnn_1515tanh512_checkpoint{}_{}'.format(trial,  iters - k)\n",
    "            if task == 'scale':\n",
    "                score = Test(task, Task.game, weight = weight_read, size = 50, test_size = 2, limit_set = 4, step = 3)\n",
    "            elif (task == 'scale_x') or (task == 'scale_y'):\n",
    "                score = Test(task, Task.game, weight = weight_read, test_size = 1, limit_set = 4)\n",
    "            else:\n",
    "                score = Test(task, Task.game, weight = weight_read)\n",
    "            Scores[i, k]= score\n",
    "        i += 1\n",
    "np.save('Scores_modules', Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "759px",
    "left": "0px",
    "right": "1228px",
    "top": "67px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
