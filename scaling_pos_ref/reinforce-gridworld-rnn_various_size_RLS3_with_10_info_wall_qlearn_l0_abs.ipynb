{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#quick-start\" data-toc-modified-id=\"quick-start-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>quick start</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Resources</a></span></li></ul></li><li><span><a href=\"#FULL-MODEL\" data-toc-modified-id=\"FULL-MODEL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>FULL MODEL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Qnetwork\" data-toc-modified-id=\"Qnetwork-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Qnetwork</a></span></li></ul></li><li><span><a href=\"#POMDP-RNN-Game\" data-toc-modified-id=\"POMDP-RNN-Game-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>POMDP RNN Game</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-setting：-grid-=--(3,7)，-holes-=-0\" data-toc-modified-id=\"Standard-setting：-grid-=--(3,7)，-holes-=-0-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Standard setting： grid =  (3,7)， holes = 0</a></span></li><li><span><a href=\"#Model-Tranining\" data-toc-modified-id=\"Model-Tranining-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Model Tranining</a></span></li><li><span><a href=\"#decoding-vs-performance\" data-toc-modified-id=\"decoding-vs-performance-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>decoding vs performance</a></span></li><li><span><a href=\"#learning-rate-vs-performance\" data-toc-modified-id=\"learning-rate-vs-performance-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>learning rate vs performance</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Anlytic-part-,-check-the-behaviour-correspond-to-each-decoding-level-and-explain-why\" data-toc-modified-id=\"Anlytic-part-,-check-the-behaviour-correspond-to-each-decoding-level-and-explain-why-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span>Anlytic part , check the behaviour correspond to each decoding level and explain why</a></span></li></ul></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>PCA</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network has perfoerct information(relative position),  with position and etc as inputs, try to see its generalization performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [*The* Reinforcement learning book from Sutton & Barto](http://incompleteideas.net/sutton/book/the-book-2nd.html)\n",
    "* [The REINFORCE paper from Ronald J. Williams (1992)](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cruiser/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "\n",
    "import pretrain\n",
    "from pretrain import *\n",
    "\n",
    "import navigation2\n",
    "from navigation2 import *\n",
    "\n",
    "%pylab inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qnetwork\n",
    "\n",
    "To select actions we take maximum of Q value, corresponding to certain move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the liquid state approach to work, you need a lot of neurons as surplus or enough hidden to hidden connectivity to make it have an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  POMDP RNN Game\n",
    "\n",
    "In this game , we use a new reward function determined by game, if the agent achieves the goal before 50, reward is 1. If time pass 50 reward is 0.5, once time pass 100 agent gets a reward of -0.5 .  Practically, this is found to be easier to learn than the rewards as a continous function of time.  Tf the agent learns to search in a efficient way, the largest possible way for search is to firstly arrive at corner then goes to the goal, which, takes about 50 steps, it is reasonble to make 50 and 100 as milestone thing.  Also in principe as the game doesn't have a timer , it is not if it can use a reward as funtion of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 condition for ending , when pass time limit, game over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For weight update, it seems to be better do it after episode, as it makes non-sense evaluate strategy during episode, but a the end. Also, it is much quicker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming of MDP here, hidden state is as state of enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pregame = PretrainGame(grid_size = (15, 15), holes = 0, random_seed = 4 , set_reward = [(0.5, 0.25), (0.5, 0.75)])\n",
    "pregame.reset(set_agent=(2,2))\n",
    "# rls_q = RLS(1)\n",
    "# rls_sl = RLS(1)\n",
    "# for i in range(1):\n",
    "#     pregame.fulltrain(trials = 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ValueMaxGame(pregame.net, grid_size = (15, 15), holes = 0, random_seed = 4 , set_reward =  [(0.5, 0.25), (0.5, 0.75)])\n",
    "game.reset()\n",
    "# game.experiment(rls_q, rls_sl, 20, epsilon = 0.5, lr = 1e-3, train_hidden = False, train_q = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8c12134a8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff920546748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(game.grid.grid)\n",
    "# plt.savefig('g16h3-map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tranining \n",
    "Pretranining is done with fixed size 15,  training is between 10 to 15, test on 19 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training from zero seems to be better because it will allow the agent to explore from new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complete experiment including pretraining , decoding training, and q learning  \n",
    "class PretrainTest():\n",
    "    def __init__(self, weight_write, holes = 0, inputs_type = (1, 0), eta = 0):\n",
    "        self.pregame = PretrainGame(grid_size = (15, 15), holes = holes, random_seed = 4 , set_reward = [(0.5, 0.25), (0.5, 0.75)], input_type = inputs_type[0])\n",
    "        self.game = ValueMaxGame(grid_size = (15, 15), holes = 0, random_seed = 4 , set_reward = [(0.5, 0.25), (0.5, 0.75)], \\\n",
    "                                 input_type = inputs_type[1], eta = eta)\n",
    "        self.weight = weight_write\n",
    "  # load the necessary weights into network             \n",
    "    def loadweight(self, weight_load):\n",
    "#       need to take the state dict as a new dict for updating , here is only tensor of trained net\n",
    "        net_dict = torch.load(weight_load)\n",
    "        # ordered list to laod\n",
    "        list_modules = [('h2h', net_dict['h2h']), ('a2h', net_dict['a2h']), ('i2h', net_dict['i2h']), ('r2h', net_dict['r2h']), ('bh', net_dict['bh'])]\n",
    "        select_dict = OrderedDict(list_modules)\n",
    "        # get the game net tensor as target\n",
    "        net = self.game.net.state_dict()\n",
    "        # update the network tensor\n",
    "        net.update(select_dict)\n",
    "        # relaod the new torsor\n",
    "        self.game.net.load_state_dict(net)\n",
    "        # save to place for update \n",
    "        torch.save(self.game.net.state_dict(), self.weight) \n",
    "    \n",
    "        \n",
    "    def pretrain(self, trial, weight = None, lr = 1e-5, pretrain = True):  \n",
    "        # start a pretrained game  \n",
    "        self.pregame.net.cuda()\n",
    "        if pretrain == True:\n",
    "            lr = float(lr)\n",
    "            if weight != None:\n",
    "                self.pregame.net.load_state_dict(torch.load(weight))\n",
    "            self.pregame.fulltrain(lr_rate = lr, trials = int(1e3), batchsize = 4)\n",
    "        print ('pretrain end', torch.norm(self.pregame.net.h2h))\n",
    "        if pretrain == True:\n",
    "            torch.save(self.pregame.net.state_dict(), self.weight[:-1]+'{}'.format(trial))\n",
    "        else:\n",
    "            torch.save(self.pregame.net.state_dict(), self.weight+'{}'.format(trial))\n",
    "        if pretrain == True and trial <= 10:\n",
    "            self.weight = self.weight[:-1]+'{}'.format(trial)\n",
    "        elif pretrain == True and trial > 10:\n",
    "            self.weight = self.weight[:-2]+'{}'.format(trial)\n",
    "        elif pretrain == False:\n",
    "            self.weight = self.weight +'{}'.format(trial)\n",
    "            \n",
    "    def decode(self, weight = None, size_range = np.arange(10, 21, 1), size_test = [10, 20], epsilon = 0):\n",
    "        if weight != None:\n",
    "            self.game.net.load_state_dict(torch.load(weight))\n",
    "        else:\n",
    "            self.pregame.net.load_state_dict(torch.load(self.weight))\n",
    "            self.game.net= self.pregame.net.cpu()\n",
    "        rls_q = RLS(1e2)\n",
    "        rls_sl = RLS(1e2)\n",
    "        self.game.net.cpu()\n",
    "        self.game.experiment(rls_q, rls_sl, iterations = 160, epsilon = epsilon, train_hidden = False, train_q = False, size_range = size_range, test = True, decode = True) \n",
    "        # tested on size 15\n",
    "        def precision(size = 15):\n",
    "            prec0 = np.mean(decodetest(self.game, reward_control= 0, epsilon = epsilon)[0] + decodetest(self.game, reward_control = 1,  epsilon = 0, size = size)[0])\n",
    "#             prec1 = np.mean(decodetest(self.game, reward_control= 0, epsilon = 0.5)[0] + decodetest(self.game, reward_control =  1,  epsilon = 0.5)[0])\n",
    "#             prec1 = np.mean(decodetest(self.game, reward_control= 0, epsilon = 1)[0] + decodetest(self.game, reward_control =  1,  epsilon = 1, size = size)[0])\n",
    "            return prec0\n",
    "        print ('decode train finish')\n",
    "        if len(size_test) == 2:\n",
    "            Prec_s = precision(size = size_test[0])\n",
    "            Prec_l = precision(size = size_test[1])\n",
    "            print ('decode end', Prec_s, Prec_l)\n",
    "            return Prec_s, Prec_l\n",
    "        elif len(size_test) == 1:\n",
    "            Prec_s = precision(size = size_test[0])\n",
    "            print ('decode end', Prec_s)\n",
    "            return Prec_s\n",
    "        # q learning session \n",
    "        # q learning session \n",
    "    \n",
    "        \n",
    "    def qlearn(self, weight_read, weight_write, iterations = 5, save = True, size_train = np.arange(10, 51, 10), size_test = [10, 30], train_only = False, test_only = False, noise = 0.3):\n",
    "        self.game.net.load_state_dict(torch.load(weight_read))\n",
    "        e_rate = [noise for r in range(iterations)] \n",
    "        rls_q = RLS(1e2)\n",
    "        rls_sl = RLS(1e2)\n",
    "        Rewards = []\n",
    "        # q leanring phase\n",
    "        for n,e in enumerate(e_rate):\n",
    "            prob = np.ones(len(size_train)) \n",
    "            prob = prob/np.sum(prob)\n",
    "            if test_only == False:\n",
    "                self.game.experiment(rls_q, rls_sl, iterations = 50, epochs= 10, epsilon = e, size_range = size_train)    \n",
    "                if save == True:\n",
    "                    torch.save(self.game.net.state_dict(), weight_write + '_{}'.format(n))\n",
    "            def testing(game):\n",
    "                Rewards00 = Test(game, reward_control = 0, size = size_test[0], test = 1)\n",
    "                Rewards01 = Test(game, reward_control = 1, size = size_test[0], test = 1)\n",
    "                rewards_s = (np.sum(Rewards00) + np.sum(Rewards01))/2\n",
    "                Rewards10 = Test(game, reward_control = 0, size = size_test[1], test = 2)\n",
    "                Rewards11 = Test(game, reward_control = 1, size = size_test[1], test = 2)\n",
    "                rewards_l = (np.sum(Rewards10) + np.sum(Rewards11))/2\n",
    "                return rewards_s, rewards_l\n",
    "            # load weight if test only is true \n",
    "            if test_only == True:\n",
    "                self.game.net.load_state_dict(torch.load(weight_write))\n",
    "            if train_only == False:\n",
    "                rewards_s, rewards_l = testing(self.game)\n",
    "            print (n, 'rewards_s',  rewards_s, 'rewards_l', rewards_l)\n",
    "            Rewards.append((rewards_s, rewards_l))\n",
    "        return Rewards\n",
    "    \n",
    "    def TestAllSizes(self, weight_read, size_range = np.arange(5, 50, 5)):\n",
    "        self.pregame.net.load_state_dict(torch.load(weight_read))\n",
    "        self.game.net.cpu()\n",
    "        self.Performance = []\n",
    "        for size in size_range:\n",
    "            Rewards0 = Test(self.game, reward_control = 0, size = size)\n",
    "            Rewards1 = Test(self.game, reward_control = 1, size = size)\n",
    "            self.Performance.append((Rewards0 + Rewards1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 159 2437271552\n",
      "decode train finish\n",
      "decode end 0.20818470155720198\n"
     ]
    }
   ],
   "source": [
    "trial = 0\n",
    "Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu/rnn_1515tanh512_checkpoint0', inputs_type=(1, 1))\n",
    "Pretest.loadweight(Pretest.weight)\n",
    "weight = 'weights_cpu/rnn_1515tanh512_checkpoint{}'.format(trial)\n",
    "Prec = Pretest.decode(weight = weight, size_range = [15], size_test = [15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 159 2437632000\n",
      "decode train finish\n",
      "decode end 0.12937955252363179\n"
     ]
    }
   ],
   "source": [
    "trial = 0\n",
    "Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu/rnn_1515tanh512_checkpoint0', inputs_type=(1, 0))\n",
    "Pretest.loadweight(Pretest.weight)\n",
    "weight = 'weights_cpu/rnn_1515tanh512_checkpoint{}'.format(trial)\n",
    "Prec = Pretest.decode(weight = weight, size_range = [15], size_test = [15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 11872083968\n",
      "0 rewards_s 0.390830963326 rewards_l -0.423678869409\n",
      "clear session data 49 11872231424\n",
      "1 rewards_s 0.436225948114 rewards_l -0.399544269102\n",
      "clear session data 49 11872256000\n",
      "2 rewards_s 0.62916094797 rewards_l -0.540625037733\n",
      "clear session data 49 11872256000\n",
      "3 rewards_s 0.728031415995 rewards_l -0.566300039004\n",
      "clear session data 49 11872256000\n",
      "4 rewards_s 0.580971783205 rewards_l -0.211610153698\n",
      "clear session data 49 11872256000\n",
      "5 rewards_s 0.53183148714 rewards_l -0.363908274474\n",
      "clear session data 49 11872301056\n",
      "6 rewards_s 0.511274177534 rewards_l -0.287300342776\n",
      "clear session data 49 11872301056\n",
      "7 rewards_s 0.62531386424 rewards_l -0.280478106139\n",
      "clear session data 49 11872301056\n",
      "8 rewards_s 0.627706432413 rewards_l -0.583890532026\n",
      "clear session data 49 11872301056\n",
      "9 rewards_s 0.600464582932 rewards_l -0.57040429385\n",
      "clear session data 49 11872301056\n",
      "0 rewards_s 0.594432359715 rewards_l -0.631091545504\n",
      "clear session data 49 11872301056\n",
      "1 rewards_s 0.677214691612 rewards_l -0.281081379989\n",
      "clear session data 49 11872301056\n",
      "2 rewards_s 0.903429279291 rewards_l -0.567892209091\n",
      "clear session data 49 11872301056\n",
      "3 rewards_s 0.844494203122 rewards_l -0.763313609467\n",
      "clear session data 49 11872301056\n",
      "4 rewards_s 0.931143385424 rewards_l -0.597633136095\n",
      "clear session data 49 11872301056\n",
      "5 rewards_s 0.904160584965 rewards_l -0.0170927108796\n",
      "clear session data 49 11872301056\n",
      "6 rewards_s 0.845307659081 rewards_l -0.556922121572\n",
      "clear session data 49 11872301056\n",
      "7 rewards_s 0.925847585035 rewards_l -0.326812006342\n",
      "clear session data 49 11872301056\n",
      "8 rewards_s 0.880102670463 rewards_l -0.54477933925\n",
      "clear session data 49 11872301056\n",
      "9 rewards_s 0.880301049827 rewards_l -0.615384615385\n",
      "clear session data 49 11872301056\n",
      "0 rewards_s 0.364201903038 rewards_l -0.66808848567\n",
      "clear session data 49 11872563200\n",
      "1 rewards_s 0.500980508312 rewards_l -0.445404262099\n",
      "clear session data 49 11872301056\n",
      "2 rewards_s 0.762518657985 rewards_l -0.476452829152\n",
      "clear session data 49 11872301056\n",
      "3 rewards_s 0.638028400564 rewards_l -0.283916219459\n",
      "clear session data 49 11872301056\n",
      "4 rewards_s 0.703528910675 rewards_l -0.59535325023\n",
      "clear session data 49 11892842496\n",
      "5 rewards_s 0.694303525078 rewards_l -0.0601047675247\n",
      "clear session data 49 11892998144\n",
      "6 rewards_s 0.722740562542 rewards_l -0.00785523237505\n",
      "clear session data 49 11892998144\n",
      "7 rewards_s 0.790900501846 rewards_l -0.420328435263\n",
      "clear session data 49 11953549312\n",
      "8 rewards_s 0.748268155542 rewards_l -0.633733538571\n",
      "clear session data 49 11954016256\n",
      "9 rewards_s 0.760999182835 rewards_l -0.598738186459\n",
      "clear session data 49 11953754112\n",
      "0 rewards_s 0.367747927339 rewards_l -0.58288228188\n",
      "clear session data 49 11953754112\n",
      "1 rewards_s 0.586330925052 rewards_l -0.53918544818\n",
      "clear session data 49 11953754112\n",
      "2 rewards_s 0.509449193393 rewards_l -0.186774695232\n",
      "clear session data 49 11953754112\n",
      "3 rewards_s 0.596274122807 rewards_l -0.245220747254\n",
      "clear session data 49 11953754112\n",
      "4 rewards_s 0.655779151404 rewards_l -0.346891016566\n",
      "clear session data 49 11953754112\n",
      "5 rewards_s 0.658585373132 rewards_l -0.381119082062\n",
      "clear session data 49 11953754112\n",
      "6 rewards_s 0.592883431713 rewards_l -0.0423922908755\n",
      "clear session data 49 11953754112\n",
      "7 rewards_s 0.743772768683 rewards_l -0.353090629514\n",
      "clear session data 49 11953754112\n",
      "8 rewards_s 0.8456464599 rewards_l -0.200473366568\n",
      "clear session data 49 11953754112\n",
      "9 rewards_s 0.660981903288 rewards_l -0.0133258776482\n",
      "clear session data 49 11953754112\n",
      "0 rewards_s 0.471405038267 rewards_l -0.523576800047\n",
      "clear session data 49 11953754112\n",
      "1 rewards_s 0.555062728938 rewards_l -0.53999035035\n",
      "clear session data 49 11953754112\n",
      "2 rewards_s 0.626104829403 rewards_l -0.590483748417\n",
      "clear session data 49 11954016256\n",
      "3 rewards_s 0.630956259427 rewards_l -0.184097361082\n",
      "clear session data 49 11954016256\n",
      "4 rewards_s 0.551336808208 rewards_l -0.464799428177\n",
      "clear session data 49 11954016256\n",
      "5 rewards_s 0.622808848101 rewards_l -0.31226246704\n",
      "clear session data 49 11954016256\n",
      "6 rewards_s 0.708654606273 rewards_l -0.0502971764425\n",
      "clear session data 49 11954016256\n",
      "7 rewards_s 0.491710763093 rewards_l -0.12891852955\n",
      "clear session data 49 11954016256\n",
      "8 rewards_s 0.620869442563 rewards_l -0.135581714448\n",
      "clear session data 49 11954016256\n",
      "9 rewards_s 0.455377491188 rewards_l -0.28060057529\n"
     ]
    }
   ],
   "source": [
    "for iters, noise in enumerate(5 * [0.0]):\n",
    "    for trial in [0]: \n",
    "        Pretest =  PretrainTest(holes = 0, inputs_type = (1, 0), weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial), eta = 0)\n",
    "        Pretest.loadweight(Pretest.weight)\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights2/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 10, noise = noise, size_train = np.arange(10, 51, 10), size_test=[10, 50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "759px",
    "left": "0px",
    "right": "1228px",
    "top": "67px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
