{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#quick-start\" data-toc-modified-id=\"quick-start-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>quick start</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Resources</a></span></li></ul></li><li><span><a href=\"#FULL-MODEL\" data-toc-modified-id=\"FULL-MODEL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>FULL MODEL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Qnetwork\" data-toc-modified-id=\"Qnetwork-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Qnetwork</a></span></li></ul></li><li><span><a href=\"#POMDP-RNN-Game\" data-toc-modified-id=\"POMDP-RNN-Game-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>POMDP RNN Game</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-setting：-grid-=--(3,7)，-holes-=-0\" data-toc-modified-id=\"Standard-setting：-grid-=--(3,7)，-holes-=-0-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Standard setting： grid =  (3,7)， holes = 0</a></span></li><li><span><a href=\"#Model-Tranining\" data-toc-modified-id=\"Model-Tranining-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Model Tranining</a></span></li><li><span><a href=\"#decoding-vs-performance\" data-toc-modified-id=\"decoding-vs-performance-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>decoding vs performance</a></span></li><li><span><a href=\"#learning-rate-vs-performance\" data-toc-modified-id=\"learning-rate-vs-performance-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>learning rate vs performance</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Anlytic-part-,-check-the-behaviour-correspond-to-each-decoding-level-and-explain-why\" data-toc-modified-id=\"Anlytic-part-,-check-the-behaviour-correspond-to-each-decoding-level-and-explain-why-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span>Anlytic part , check the behaviour correspond to each decoding level and explain why</a></span></li></ul></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>PCA</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test what is the decoding change for single size training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [*The* Reinforcement learning book from Sutton & Barto](http://incompleteideas.net/sutton/book/the-book-2nd.html)\n",
    "* [The REINFORCE paper from Ronald J. Williams (1992)](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tie/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "import pretrain\n",
    "from pretrain import *\n",
    "\n",
    "import Nets\n",
    "from Nets import*\n",
    "\n",
    "import navigation2\n",
    "from navigation2 import *\n",
    "\n",
    "%pylab inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qnetwork\n",
    "\n",
    "To select actions we take maximum of Q value, corresponding to certain move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the liquid state approach to work, you need a lot of neurons as surplus or enough hidden to hidden connectivity to make it have an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  POMDP RNN Game\n",
    "\n",
    "In this game , we use a new reward function determined by game, if the agent achieves the goal before 50, reward is 1. If time pass 50 reward is 0.5, once time pass 100 agent gets a reward of -0.5 .  Practically, this is found to be easier to learn than the rewards as a continous function of time.  Tf the agent learns to search in a efficient way, the largest possible way for search is to firstly arrive at corner then goes to the goal, which, takes about 50 steps, it is reasonble to make 50 and 100 as milestone thing.  Also in principe as the game doesn't have a timer , it is not if it can use a reward as funtion of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 condition for ending , when pass time limit, game over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For weight update, it seems to be better do it after episode, as it makes non-sense evaluate strategy during episode, but a the end. Also, it is much quicker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming of MDP here, hidden state is as state of enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pregame = PretrainGame(grid_size = (15, 15), holes = 0, random_seed = 4 , set_reward = [(0.5, 0.25), (0.5, 0.75)])\n",
    "pregame.reset(set_agent=(2,2))\n",
    "# rls_q = RLS(1)\n",
    "# rls_sl = RLS(1)\n",
    "# for i in range(1):\n",
    "#     pregame.fulltrain(trials = 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ValueMaxGame(pregame.net, grid_size = (15, 15), holes = 0, random_seed = 4 , set_reward =  [(0.5, 0.25), (0.5, 0.75)])\n",
    "game.reset()\n",
    "# game.experiment(rls_q, rls_sl, 20, epsilon = 0.5, lr = 1e-3, train_hidden = False, train_q = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f593a0b2fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59aeaef1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(game.grid.grid)\n",
    "# plt.savefig('g16h3-map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tranining \n",
    "Pretranining is done with fixed size 15,  training is between 10 to 15, test on 19 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training from zero seems to be better because it will allow the agent to explore from new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 2845798400\n",
      "0 rewards 0.083221916971917\n",
      "clear session data 49 2845798400\n",
      "1 rewards 0.7079337194331117\n",
      "clear session data 49 2845798400\n",
      "2 rewards 0.7616159774135186\n",
      "clear session data 49 2845798400\n",
      "3 rewards 0.7991116955960706\n",
      "clear session data 49 2845798400\n",
      "4 rewards 0.6656408836959824\n",
      "clear session data 49 2845798400\n",
      "5 rewards 0.7453311557960405\n",
      "clear session data 49 2845798400\n",
      "6 rewards 0.566066053278829\n",
      "clear session data 49 2845798400\n",
      "7 rewards 0.5604295322328252\n",
      "clear session data 49 2845798400\n",
      "8 rewards 0.6859113474245053\n",
      "clear session data 49 2845798400\n",
      "9 rewards 0.742012934981685\n",
      "clear session data 49 2845798400\n",
      "10 rewards 0.7241978363072112\n",
      "clear session data 49 2845798400\n",
      "11 rewards 0.647302668029188\n",
      "clear session data 49 2845798400\n",
      "12 rewards 0.5766344848578147\n",
      "clear session data 49 2845798400\n",
      "13 rewards 0.7027503920006188\n",
      "clear session data 49 2845798400\n",
      "14 rewards 0.6565962230920768\n",
      "clear session data 49 2845798400\n",
      "15 rewards 0.6981724221958596\n",
      "clear session data 49 2845798400\n",
      "16 rewards 0.8407248719390304\n",
      "clear session data 49 2846023680\n",
      "17 rewards 0.7877997263153513\n",
      "clear session data 49 2846023680\n",
      "18 rewards 0.7467181654532902\n",
      "clear session data 49 2846023680\n",
      "19 rewards 0.6512728435672515\n",
      "clear session data 49 2878185472\n",
      "0 rewards -0.5826388888888889\n",
      "clear session data 49 2878185472\n",
      "1 rewards -0.29122034131810953\n",
      "clear session data 49 2878185472\n",
      "2 rewards -0.03526041666666667\n",
      "clear session data 49 2878185472\n",
      "3 rewards -0.08481800766283526\n",
      "clear session data 49 2878185472\n",
      "4 rewards 0.3729938553444993\n",
      "clear session data 49 2878185472\n",
      "5 rewards 0.5411662787155499\n",
      "clear session data 49 2878185472\n",
      "6 rewards 0.5491062474428994\n",
      "clear session data 49 2878185472\n",
      "7 rewards 0.7150311327326546\n",
      "clear session data 49 2878185472\n",
      "8 rewards 0.6782643592960689\n",
      "clear session data 49 2878185472\n",
      "9 rewards 0.7466100148008752\n",
      "clear session data 49 2878185472\n",
      "10 rewards 0.6924000848251778\n",
      "clear session data 49 2878185472\n",
      "11 rewards 0.6025770635740065\n",
      "clear session data 49 2878185472\n",
      "12 rewards 0.6321474400302673\n",
      "clear session data 49 2878185472\n",
      "13 rewards 0.8126701780922466\n",
      "clear session data 49 2878185472\n",
      "14 rewards 0.7007653053247248\n",
      "clear session data 49 2878185472\n",
      "15 rewards 0.7018723794464634\n",
      "clear session data 49 2878185472\n",
      "16 rewards 0.6880688664864304\n",
      "clear session data 49 2878455808\n",
      "17 rewards 0.6477795757483258\n",
      "clear session data 49 2878455808\n",
      "18 rewards 0.7448808582633422\n",
      "clear session data 49 2878455808\n",
      "19 rewards 0.5625833634743047\n",
      "clear session data 49 2913325056\n",
      "0 rewards 0.113064937138213\n",
      "clear session data 49 2913325056\n",
      "1 rewards 0.1867296854588057\n",
      "clear session data 49 2913325056\n",
      "2 rewards 0.04624490093240094\n",
      "clear session data 49 2913325056\n",
      "3 rewards -0.22362689393939394\n",
      "clear session data 49 2913325056\n",
      "4 rewards -0.21624451465653913\n",
      "clear session data 49 2913325056\n",
      "5 rewards 0.5528221276982466\n",
      "clear session data 49 2913325056\n",
      "6 rewards 0.6775702898035872\n",
      "clear session data 49 2913325056\n",
      "7 rewards 0.603020252641248\n",
      "clear session data 49 2913325056\n",
      "8 rewards 0.7687943723419983\n",
      "clear session data 49 2913325056\n",
      "9 rewards 0.7730711447117697\n",
      "clear session data 49 2913325056\n",
      "10 rewards 0.7843721466459954\n",
      "clear session data 49 2913325056\n",
      "11 rewards 0.6458494455415136\n",
      "clear session data 49 2913325056\n",
      "12 rewards 0.6388979756773399\n",
      "clear session data 49 2913325056\n",
      "13 rewards 0.6729588510447886\n",
      "clear session data 49 2913480704\n",
      "14 rewards 0.7835061190610391\n",
      "clear session data 49 2913480704\n",
      "15 rewards 0.8255279907693134\n",
      "clear session data 49 2913480704\n",
      "16 rewards 0.5379323997110921\n",
      "clear session data 49 2913480704\n",
      "17 rewards 0.5896195211038961\n",
      "clear session data 49 2913480704\n",
      "18 rewards 0.5839885317017477\n",
      "clear session data 49 2913480704\n",
      "19 rewards 0.4667729595638803\n",
      "clear session data 49 2861789184\n",
      "0 rewards -0.2971789856555481\n",
      "clear session data 49 2861789184\n",
      "1 rewards 0.4267040478578988\n",
      "clear session data 49 2861789184\n",
      "2 rewards 0.5435546451659756\n",
      "clear session data 49 2861789184\n",
      "3 rewards 0.7910834910359368\n",
      "clear session data 49 2861789184\n",
      "4 rewards 0.8494557375620555\n",
      "clear session data 49 2861789184\n",
      "5 rewards 0.7288058670157269\n",
      "clear session data 49 2861789184\n",
      "6 rewards 0.9201597447691198\n",
      "clear session data 49 2861789184\n",
      "7 rewards 0.8690969209343637\n",
      "clear session data 49 2861789184\n",
      "8 rewards 0.8814761935393025\n",
      "clear session data 49 2861789184\n",
      "9 rewards 0.8751593216908411\n",
      "clear session data 49 2861789184\n",
      "10 rewards 0.814527483207582\n",
      "clear session data 49 2861789184\n",
      "11 rewards 0.8883885438233264\n",
      "clear session data 49 2861789184\n",
      "12 rewards 0.6506176631378678\n",
      "clear session data 49 2861789184\n",
      "13 rewards 0.4649943794291221\n",
      "clear session data 49 2861789184\n",
      "14 rewards 0.6059163150446045\n",
      "clear session data 49 2861789184\n",
      "15 rewards 0.6829327490495193\n",
      "clear session data 49 2861789184\n",
      "16 rewards 0.7094363637396808\n",
      "clear session data 49 2861789184\n",
      "17 rewards 0.668280392530586\n",
      "clear session data 49 2861789184\n",
      "18 rewards 0.6164566967655658\n",
      "clear session data 49 2861789184\n",
      "19 rewards 0.7197744946634749\n",
      "clear session data 49 2866384896\n",
      "0 rewards -0.03724585499908775\n",
      "clear session data 49 2866384896\n",
      "1 rewards 0.3502939116953663\n",
      "clear session data 49 2866384896\n",
      "2 rewards 0.7248008829667887\n",
      "clear session data 49 2866655232\n",
      "3 rewards 0.6461047935853501\n",
      "clear session data 49 2866655232\n",
      "4 rewards 0.5447325244200244\n",
      "clear session data 49 2866655232\n",
      "5 rewards 0.686141945313141\n",
      "clear session data 49 2866655232\n",
      "6 rewards 0.5876807870948496\n",
      "clear session data 49 2866655232\n",
      "7 rewards 0.47822975635475634\n",
      "clear session data 49 2866655232\n",
      "8 rewards 0.37142198079698074\n",
      "clear session data 49 2866655232\n",
      "9 rewards 0.4200006157037407\n",
      "clear session data 49 2866704384\n",
      "10 rewards 0.42337538763320015\n",
      "clear session data 49 2866704384\n",
      "11 rewards 0.34487483003108\n",
      "clear session data 49 2866704384\n",
      "12 rewards 0.5485114278083028\n",
      "clear session data 49 2866704384\n",
      "13 rewards 0.3622513337357087\n",
      "clear session data 49 2866704384\n",
      "14 rewards 0.38888988427031906\n",
      "clear session data 49 2866704384\n",
      "15 rewards 0.35060767357642364\n",
      "clear session data 49 2866704384\n",
      "16 rewards 0.3841575525169275\n",
      "clear session data 49 2866704384\n",
      "17 rewards 0.3370510132228882\n",
      "clear session data 49 2866704384\n",
      "18 rewards 0.3613129447181171\n",
      "clear session data 49 2866704384\n",
      "19 rewards 0.3353149843451567\n",
      "clear session data 49 2866704384\n",
      "0 rewards -0.009176517693178357\n",
      "clear session data 49 2866704384\n",
      "1 rewards 0.4022882890308162\n",
      "clear session data 49 2866704384\n",
      "2 rewards 0.6966562448881528\n",
      "clear session data 49 2866704384\n",
      "3 rewards 0.6352137866913683\n",
      "clear session data 49 2866704384\n",
      "4 rewards 0.7007097647722648\n",
      "clear session data 49 2866704384\n",
      "5 rewards 0.7109050178389892\n",
      "clear session data 49 2866704384\n",
      "6 rewards 0.8212351463984551\n",
      "clear session data 49 2866704384\n",
      "7 rewards 0.746023519880249\n",
      "clear session data 49 2866704384\n",
      "8 rewards 0.6459253383032624\n",
      "clear session data 49 2866704384\n",
      "9 rewards 0.6515626943698065\n",
      "clear session data 49 2866704384\n",
      "10 rewards 0.7430560707674678\n",
      "clear session data 49 2866704384\n",
      "11 rewards 0.5408147961353781\n",
      "clear session data 49 2866704384\n",
      "12 rewards 0.5426053111092052\n",
      "clear session data 49 2866704384\n",
      "13 rewards 0.6277102498196248\n",
      "clear session data 49 2866704384\n",
      "14 rewards 0.5557333870712029\n",
      "clear session data 49 2866704384\n",
      "15 rewards 0.5044327885199595\n",
      "clear session data 49 2866704384\n",
      "16 rewards 0.7259334979256854\n",
      "clear session data 49 2866704384\n",
      "17 rewards 0.6456650482041107\n",
      "clear session data 49 2866704384\n",
      "18 rewards 0.7412515367045913\n",
      "clear session data 49 2866704384\n",
      "19 rewards 0.6095918226852874\n"
     ]
    }
   ],
   "source": [
    "for iters, noise in enumerate(6 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 20, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 3111833600\n",
      "0 rewards 0.3999502149445101\n",
      "clear session data 49 3111833600\n",
      "1 rewards 0.5252749608587315\n",
      "clear session data 49 3111833600\n",
      "2 rewards 0.4749695114826694\n",
      "clear session data 49 3111833600\n",
      "3 rewards 0.4929316236681304\n",
      "clear session data 49 3111833600\n",
      "4 rewards 0.47591618450993445\n",
      "clear session data 49 3111833600\n",
      "5 rewards 0.46371843852986094\n",
      "clear session data 49 3111833600\n",
      "6 rewards 0.5849789762933144\n",
      "clear session data 49 3111833600\n",
      "7 rewards 0.49107088224275725\n",
      "clear session data 49 3111833600\n",
      "8 rewards 0.5339531477755679\n",
      "clear session data 49 3111833600\n",
      "9 rewards 0.6169878253929979\n",
      "clear session data 49 3111833600\n",
      "10 rewards 0.5154079980723301\n",
      "clear session data 49 3111833600\n",
      "11 rewards 0.5942660637973138\n",
      "clear session data 49 3111833600\n",
      "12 rewards 0.6052643505598669\n",
      "clear session data 49 3111833600\n",
      "13 rewards 0.6299902257404951\n",
      "clear session data 49 3111833600\n",
      "14 rewards 0.5251526917844149\n",
      "clear session data 49 3111833600\n",
      "15 rewards 0.44086804551442715\n",
      "clear session data 49 3111833600\n",
      "16 rewards 0.43907021074989827\n",
      "clear session data 49 3111833600\n",
      "17 rewards 0.6048625484509105\n",
      "clear session data 49 3111833600\n",
      "18 rewards 0.513076327303301\n",
      "clear session data 49 3111833600\n",
      "19 rewards 0.5064693986568987\n",
      "clear session data 49 3133755392\n",
      "0 rewards -0.31875347014346334\n",
      "clear session data 49 3133755392\n",
      "1 rewards 0.49458659343975653\n",
      "clear session data 49 3133755392\n",
      "2 rewards 0.5206955404821316\n",
      "clear session data 49 3133755392\n",
      "3 rewards 0.6124672174519321\n",
      "clear session data 49 3133755392\n",
      "4 rewards 0.6858014898722136\n",
      "clear session data 49 3133755392\n",
      "5 rewards 0.5443425830535206\n",
      "clear session data 49 3133755392\n",
      "6 rewards 0.5291454311936373\n",
      "clear session data 49 3133755392\n",
      "7 rewards 0.6456376034423359\n",
      "clear session data 49 3133755392\n",
      "8 rewards 0.4510323240891242\n",
      "clear session data 49 3133755392\n",
      "9 rewards 0.6039506066849816\n",
      "clear session data 49 3134025728\n",
      "10 rewards 0.5539407496438746\n",
      "clear session data 49 3134025728\n",
      "11 rewards 0.4867813639904157\n",
      "clear session data 49 3134025728\n",
      "12 rewards 0.5132619984182484\n",
      "clear session data 49 3134025728\n",
      "13 rewards 0.43263979943667447\n",
      "clear session data 49 3134025728\n",
      "14 rewards 0.6311829446595072\n",
      "clear session data 49 3134025728\n",
      "15 rewards 0.5558276503589004\n",
      "clear session data 49 3134025728\n",
      "16 rewards 0.5828911973443224\n",
      "clear session data 49 3134025728\n",
      "17 rewards 0.47350305306417434\n",
      "clear session data 49 3134025728\n",
      "18 rewards 0.47765672463183906\n",
      "clear session data 49 3134025728\n",
      "19 rewards 0.5033927055138797\n",
      "clear session data 49 3134025728\n",
      "0 rewards -0.01602162099764401\n",
      "clear session data 49 3134025728\n",
      "1 rewards 0.49887707943895054\n",
      "clear session data 49 3134025728\n",
      "2 rewards 0.7465239517804055\n",
      "clear session data 49 3134025728\n",
      "3 rewards 0.7094820202448604\n",
      "clear session data 49 3134025728\n",
      "4 rewards 0.7639047607180831\n",
      "clear session data 49 3134025728\n",
      "5 rewards 0.7388892912616454\n",
      "clear session data 49 3134025728\n",
      "6 rewards 0.7505477017195767\n",
      "clear session data 49 3134025728\n",
      "7 rewards 0.8121508699633699\n",
      "clear session data 49 3134025728\n",
      "8 rewards 0.7745115543778827\n",
      "clear session data 49 3134025728\n",
      "9 rewards 0.7299466124610039\n",
      "clear session data 49 3134025728\n",
      "10 rewards 0.8211049230970212\n",
      "clear session data 49 3134025728\n",
      "11 rewards 0.6960503574244383\n",
      "clear session data 49 3134025728\n",
      "12 rewards 0.6794713140714019\n",
      "clear session data 49 3134025728\n",
      "13 rewards 0.6941220752374115\n",
      "clear session data 49 3134025728\n",
      "14 rewards 0.5786987542861972\n",
      "clear session data 49 3134025728\n",
      "15 rewards 0.5677285673613374\n",
      "clear session data 49 3134025728\n",
      "16 rewards 0.6298819380850631\n",
      "clear session data 49 3134025728\n",
      "17 rewards 0.6095669502512175\n",
      "clear session data 49 3134025728\n",
      "18 rewards 0.6302519371054549\n",
      "clear session data 49 3134025728\n",
      "19 rewards 0.5838066302910053\n",
      "clear session data 49 3134836736\n",
      "0 rewards -0.2638226356976357\n",
      "clear session data 49 3153756160\n",
      "1 rewards 0.04011873925706645\n",
      "clear session data 49 3153756160\n",
      "2 rewards 0.15155528830850523\n",
      "clear session data 49 3153756160\n",
      "3 rewards 0.29761869389747675\n",
      "clear session data 49 3153756160\n",
      "4 rewards 0.5757625065208578\n",
      "clear session data 49 3153756160\n",
      "5 rewards 0.6223332339954106\n",
      "clear session data 49 3153756160\n",
      "6 rewards 0.6661425661762408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f2f6e9747b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mweight_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mweight_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_read\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweight_write\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards_{}_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/Nets.py\u001b[0m in \u001b[0;36mqlearn\u001b[0;34m(self, weight_read, weight_write, iterations, save, size_train, size_test, train_only, test_only, noise, h2o)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_only\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_write\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self, rls_q, rls_sl, iterations, epochs, epsilon, num_episodes, reward_control, train_hidden, train_q, decode, size_range, test)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_control\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_q\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self, epochs, epsilon, reward_control, size_range, prob, train_hidden, test, decode)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_t0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0mDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, policy, epsilon, record, test, cross, train_hidden, decode)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# action to state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mstate_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisible_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mpos1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# check stop condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iters, noise in enumerate(5 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+6)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 20, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+6, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add collision punishment\n",
    "for iters, noise in enumerate(5 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+6)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 20, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+6, trial), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measure decoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding accuracy decreases, there are two possible reasons: \n",
    "1, decoding has bias,  information of position is only refleted by decoding, if there is certain bias , it is not trustable\n",
    "2, it is the real measure of information, so there are less spatial information about space, or an effect decoupling/disentanglement of position. This can be due the network succeed at finding an even lower diemnsion object which gives successful performance.  The representation deceases its information to input in a sense. \n",
    "\n",
    "This can be linked to receptive field by looking at how space is represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 3433295872\n",
      "0 rewards -0.33081906134586087\n",
      "clear session data 49 3390676992\n",
      "1 rewards -0.05595141882183907\n",
      "clear session data 49 3390676992\n",
      "2 rewards 0.21055151455744125\n",
      "clear session data 49 3390676992\n",
      "3 rewards 0.5135825477519557\n",
      "clear session data 49 3390676992\n",
      "4 rewards 0.9084560958971526\n",
      "clear session data 49 3390676992\n",
      "5 rewards 0.7912402701465202\n",
      "clear session data 49 3390676992\n",
      "6 rewards 0.7619081093992075\n",
      "clear session data 49 3390676992\n",
      "7 rewards 0.783366037143211\n",
      "clear session data 49 3390676992\n",
      "8 rewards 0.7204172445254629\n",
      "clear session data 49 3390676992\n",
      "9 rewards 0.8494295558646195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-597809409a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mweight_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mweight_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_read\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweight_write\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards_{}_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/Nets.py\u001b[0m in \u001b[0;36mqlearn\u001b[0;34m(self, weight_read, weight_write, iterations, save, size_train, size_test, train_only, test_only, noise, h2o)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_only\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_write\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self, rls_q, rls_sl, iterations, epochs, epsilon, num_episodes, reward_control, train_hidden, train_q, decode, size_range, test)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_control\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_q\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self, epochs, epsilon, reward_control, size_range, prob, train_hidden, test, decode)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_t0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0mDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, policy, epsilon, record, test, cross, train_hidden, decode)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# record values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mTD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;31m# record position and hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mTD\u001b[0;34m(decode)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# update value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mTD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mrealQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplacefield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;31m# target Q is for state before updated, it only needs to update the value assocate with action taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtargetQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dynamical lambda not alpha\n",
    "for iters, noise in enumerate(5 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+9)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 10, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+9, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 3629940736\n",
      "0 rewards 0.46939569936492886\n",
      "clear session data 49 3629940736\n",
      "1 rewards 0.7426123992566707\n",
      "clear session data 49 3629940736\n",
      "2 rewards 0.7790225990510613\n",
      "clear session data 49 3629940736\n",
      "3 rewards 0.7850228432719242\n",
      "clear session data 49 3629940736\n",
      "4 rewards 0.7624041218423508\n",
      "clear session data 49 3629940736\n",
      "5 rewards 0.8200171476925793\n",
      "clear session data 49 3629940736\n",
      "6 rewards 0.7566214966502796\n",
      "clear session data 49 3629940736\n",
      "7 rewards 0.7118805933649683\n",
      "clear session data 49 3629940736\n",
      "8 rewards 0.631123380495783\n",
      "clear session data 49 3629940736\n",
      "9 rewards 0.5404808935277685\n",
      "clear session data 49 3591143424\n",
      "0 rewards 0.28290066555531707\n",
      "clear session data 49 3591143424\n",
      "1 rewards -0.640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f11753bfaaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mweight_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mweight_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_read\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweight_write\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards_{}_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/Nets.py\u001b[0m in \u001b[0;36mqlearn\u001b[0;34m(self, weight_read, weight_write, iterations, save, size_train, size_test, train_only, test_only, noise, h2o)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_only\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_write\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self, rls_q, rls_sl, iterations, epochs, epsilon, num_episodes, reward_control, train_hidden, train_q, decode, size_range, test)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2p_rls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp_rls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrls_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_control\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# initialize, might take data during test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self, epochs, epsilon, reward_control, size_range, prob, train_hidden, test, decode)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreward_control\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_control\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_control\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, policy, epsilon, record, test, cross, train_hidden, decode)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# update all last action values with eligibility trace, the q will add a new updated value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;31m#             print (e, delta, q)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mTD\u001b[0;34m(decode)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# death\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;31m# Check if agent won (reached the goal) or lost (health reached 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# attention! 需要括号， 否则reward会被更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/RNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, action, reward)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mhidden_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_action\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dynamical lambda not alpha, action changed no wall not fed back\n",
    "for iters, noise in enumerate(5 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+10)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 10, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+10, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 2867220480\n",
      "0 rewards 0.4387704060833173\n",
      "clear session data 49 2867535872\n",
      "1 rewards 0.7180914932670489\n",
      "clear session data 49 2867535872\n",
      "2 rewards 0.7162155507925982\n",
      "clear session data 49 2867535872\n",
      "3 rewards 0.7858049687276043\n",
      "clear session data 49 2867535872\n",
      "4 rewards 0.6900069319033665\n",
      "clear session data 49 2867535872\n",
      "5 rewards 0.6663131964145887\n",
      "clear session data 49 2867535872\n",
      "6 rewards 0.7408936633283238\n",
      "clear session data 49 2867535872\n",
      "7 rewards 0.6811684496059496\n",
      "clear session data 49 2867535872\n",
      "8 rewards 0.6268936336746284\n",
      "clear session data 49 2867535872\n",
      "9 rewards 0.6668250624313218\n",
      "clear session data 49 2869669888\n",
      "0 rewards 0.2662915148576913\n",
      "clear session data 49 2869669888\n",
      "1 rewards 0.8300123864982916\n",
      "clear session data 49 2869669888\n",
      "2 rewards 0.7655145648073989\n",
      "clear session data 49 2869895168\n",
      "3 rewards 0.8568235371314414\n",
      "clear session data 49 2869895168\n",
      "4 rewards 0.7868855536824286\n",
      "clear session data 49 2869895168\n",
      "5 rewards 0.8253021110833612\n",
      "clear session data 49 2869895168\n",
      "6 rewards 0.6941248890160006\n",
      "clear session data 49 2869895168\n",
      "7 rewards 0.6042981824348954\n",
      "clear session data 49 2869895168\n",
      "8 rewards 0.54054579702332\n",
      "clear session data 49 2869895168\n",
      "9 rewards 0.5475568742977031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-83bd2226e9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mweight_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mweight_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_read\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweight_write\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards_{}_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/Nets.py\u001b[0m in \u001b[0;36mqlearn\u001b[0;34m(self, weight_read, weight_write, iterations, save, size_train, size_test, train_only, test_only, noise, h2o)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_only\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_write\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self, rls_q, rls_sl, iterations, epochs, epsilon, num_episodes, reward_control, train_hidden, train_q, decode, size_range, test)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_control\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_q\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self, epochs, epsilon, reward_control, size_range, prob, train_hidden, test, decode)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_t0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0mDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, policy, epsilon, record, test, cross, train_hidden, decode)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# record values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mTD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# record position and hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mTD\u001b[0;34m(decode)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m#         print (self.trace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;31m#         print (self.Qs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# record values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m#         print (self.trace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;31m#         print (self.Qs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# record values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(e, delta, q)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m#             print (e, delta, q)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m#         print (self.trace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# punish on the border , changed action not fedback\n",
    "for iters, noise in enumerate(5 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+11)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 10, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+11, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 3310940160\n",
      "0 rewards 0.3207760423056476\n",
      "clear session data 49 3310940160\n",
      "1 rewards 0.7458242713627543\n",
      "clear session data 49 3310940160\n",
      "2 rewards 0.7387079605021416\n",
      "clear session data 49 3310940160\n",
      "3 rewards 0.6179889268634999\n",
      "clear session data 49 3310940160\n",
      "4 rewards 0.7335096079066867\n",
      "clear session data 49 3310940160\n",
      "5 rewards 0.76000481000481\n",
      "clear session data 49 3310940160\n",
      "6 rewards 0.7052545746578613\n",
      "clear session data 49 3310940160\n",
      "7 rewards 0.6913075320643791\n",
      "clear session data 49 3310940160\n",
      "8 rewards 0.7051332794554301\n",
      "clear session data 49 3310940160\n",
      "9 rewards 0.7455490617599992\n",
      "clear session data 49 3287216128\n",
      "0 rewards 0.5285267504611935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6f4dc1f6a5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mweight_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mweight_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_read\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweight_write\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards_{}_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/Nets.py\u001b[0m in \u001b[0;36mqlearn\u001b[0;34m(self, weight_read, weight_write, iterations, save, size_train, size_test, train_only, test_only, noise, h2o)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_only\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_write\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self, rls_q, rls_sl, iterations, epochs, epsilon, num_episodes, reward_control, train_hidden, train_q, decode, size_range, test)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_control\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_q\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrls_sl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self, epochs, epsilon, reward_control, size_range, prob, train_hidden, test, decode)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_t0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0mDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, policy, epsilon, record, test, cross, train_hidden, decode)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# record values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mTD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# record position and hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mTD\u001b[0;34m(decode)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m#         print (self.trace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;31m#         print (self.Qs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# record values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m#         print (self.trace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;31m#         print (self.Qs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# record values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/PhD/NavigationPaper_v6/Graphs_cosyne/Fig_pretrain_game/navigation2.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(e, delta, q)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m#             print (e, delta, q)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m#         print (self.trace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# punish on the border , changed action not fedback, dynamical alpha \n",
    "for iters, noise in enumerate(5 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+13)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 10, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+13, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 3715334144\n",
      "0 rewards 0.4740632005497023\n",
      "clear session data 49 3715334144\n",
      "1 rewards 0.6543910590616154\n",
      "clear session data 49 3715334144\n",
      "2 rewards 0.7363924472019838\n",
      "clear session data 49 3715334144\n",
      "3 rewards 0.7232335370446785\n",
      "clear session data 49 3715334144\n",
      "4 rewards 0.8193652815568934\n",
      "clear session data 49 3715334144\n",
      "0 rewards 0.16018463292311974\n",
      "clear session data 49 3715334144\n",
      "1 rewards 0.65208442602968\n",
      "clear session data 49 3715334144\n",
      "2 rewards 0.7578395160487903\n",
      "clear session data 49 3715334144\n",
      "3 rewards 0.7516291875908749\n",
      "clear session data 49 3715334144\n",
      "4 rewards 0.71738920220809\n",
      "clear session data 49 3715334144\n",
      "0 rewards 0.5328346467407179\n",
      "clear session data 49 3715334144\n",
      "1 rewards 0.5350667106902844\n",
      "clear session data 49 3715334144\n",
      "2 rewards 0.588742258898509\n",
      "clear session data 49 3715334144\n",
      "3 rewards 0.7648959418792276\n",
      "clear session data 49 3715334144\n",
      "4 rewards 0.743379407051282\n",
      "clear session data 49 3676467200\n",
      "0 rewards 0.2896330882122049\n",
      "clear session data 49 3676467200\n",
      "1 rewards 0.5842059749863784\n",
      "clear session data 49 3676467200\n",
      "2 rewards 0.6420505159592846\n",
      "clear session data 49 3676467200\n",
      "3 rewards 0.7505994991429421\n",
      "clear session data 49 3676467200\n",
      "4 rewards 0.6889942174969583\n",
      "clear session data 49 3676467200\n",
      "0 rewards 0.5298551217301217\n",
      "clear session data 49 3676467200\n",
      "1 rewards 0.7636913087729796\n",
      "clear session data 49 3676467200\n",
      "2 rewards 0.7845756695301659\n",
      "clear session data 49 3676467200\n",
      "3 rewards 0.705398421688783\n",
      "clear session data 49 3676467200\n",
      "4 rewards 0.6957169686156754\n",
      "clear session data 49 3676467200\n",
      "0 rewards 0.1792143645296171\n",
      "clear session data 49 3676467200\n",
      "1 rewards 0.6856818412505022\n",
      "clear session data 49 3676467200\n",
      "2 rewards 0.700761154175529\n",
      "clear session data 49 3676467200\n",
      "3 rewards 0.7990493881118881\n",
      "clear session data 49 3676467200\n",
      "4 rewards 0.8072640483434845\n",
      "clear session data 49 3632234496\n",
      "0 rewards 0.26731151354772675\n",
      "clear session data 49 3632234496\n",
      "1 rewards 0.5117230664615176\n",
      "clear session data 49 3632234496\n",
      "2 rewards 0.7687646853864526\n",
      "clear session data 49 3632234496\n",
      "3 rewards 0.695791815627909\n",
      "clear session data 49 3632234496\n",
      "4 rewards 0.7126053589011563\n",
      "clear session data 49 3632234496\n",
      "0 rewards 0.2057720983717751\n",
      "clear session data 49 3632234496\n",
      "1 rewards 0.5564648543905009\n",
      "clear session data 49 3632234496\n",
      "2 rewards 0.708418342363655\n",
      "clear session data 49 3632234496\n",
      "3 rewards 0.7420500536341053\n",
      "clear session data 49 3632234496\n",
      "4 rewards 0.617501932143308\n",
      "clear session data 49 3632234496\n",
      "0 rewards 0.44739574347250977\n",
      "clear session data 49 3632234496\n",
      "1 rewards 0.5805553482277621\n",
      "clear session data 49 3632234496\n",
      "2 rewards 0.7095673032978599\n",
      "clear session data 49 3632234496\n",
      "3 rewards 0.7007797232149096\n",
      "clear session data 49 3632234496\n",
      "4 rewards 0.7004319510898458\n",
      "clear session data 49 3632234496\n",
      "0 rewards 0.47437200962132287\n",
      "clear session data 49 3632234496\n",
      "1 rewards 0.7032777976774687\n",
      "clear session data 49 3632234496\n",
      "2 rewards 0.6861973963536463\n",
      "clear session data 49 3632234496\n",
      "3 rewards 0.7098808424696641\n",
      "clear session data 49 3632234496\n",
      "4 rewards 0.8841369651573925\n"
     ]
    }
   ],
   "source": [
    "# punish on the border , changed action not fedback, dynamical alpha \n",
    "for iters, noise in enumerate(10 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+14)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 5, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+14, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear session data 49 2870112256\n",
      "0 rewards 0.4219898884306053\n",
      "clear session data 49 2870374400\n",
      "1 rewards 0.7051108714405108\n",
      "clear session data 49 2870374400\n",
      "2 rewards 0.6632439818323439\n",
      "clear session data 49 2870374400\n",
      "3 rewards 0.5668420368570293\n",
      "clear session data 49 2870374400\n",
      "4 rewards 0.7080936139427093\n",
      "clear session data 49 2870374400\n",
      "0 rewards 0.2689038825757576\n",
      "clear session data 49 2870374400\n",
      "1 rewards 0.6591888435638436\n",
      "clear session data 49 2870374400\n",
      "2 rewards 0.7952589721134129\n",
      "clear session data 49 2870439936\n",
      "3 rewards 0.7118715339854771\n",
      "clear session data 49 2870439936\n",
      "4 rewards 0.6780522624424583\n",
      "clear session data 49 2870439936\n",
      "0 rewards 0.421719749717372\n",
      "clear session data 49 2870439936\n",
      "1 rewards 0.6533666843968333\n",
      "clear session data 49 2870439936\n",
      "2 rewards 0.7155150249969329\n",
      "clear session data 49 2870439936\n",
      "3 rewards 0.7388958870589817\n",
      "clear session data 49 2870439936\n",
      "4 rewards 0.6587961877612365\n",
      "clear session data 49 2870439936\n",
      "0 rewards 0.5555466119120736\n",
      "clear session data 49 2870439936\n",
      "1 rewards 0.6072987741349327\n",
      "clear session data 49 2870439936\n",
      "2 rewards 0.7288620166650878\n",
      "clear session data 49 2870439936\n",
      "3 rewards 0.8504771452962241\n",
      "clear session data 49 2870439936\n",
      "4 rewards 0.6427100068547437\n",
      "clear session data 49 2870439936\n",
      "0 rewards 0.49281504428502787\n",
      "clear session data 49 2870439936\n",
      "1 rewards 0.6197188586571193\n",
      "clear session data 49 2870439936\n",
      "2 rewards 0.5435058821386947\n",
      "clear session data 49 2870439936\n",
      "3 rewards 0.7058087135858684\n",
      "clear session data 49 2870439936\n",
      "4 rewards 0.7516854358627783\n",
      "clear session data 49 2870439936\n",
      "0 rewards 0.1564045132323904\n",
      "clear session data 49 2870439936\n",
      "1 rewards 0.6022479883637863\n",
      "clear session data 49 2870439936\n",
      "2 rewards 0.7211817303444341\n",
      "clear session data 49 2870439936\n",
      "3 rewards 0.5935837132297188\n",
      "clear session data 49 2870439936\n",
      "4 rewards 0.7447963347082261\n",
      "clear session data 49 2895568896\n",
      "0 rewards 0.2608199783811197\n",
      "clear session data 49 2895568896\n",
      "1 rewards 0.5793385382999353\n",
      "clear session data 49 2895568896\n",
      "2 rewards 0.6613317940822158\n",
      "clear session data 49 2895568896\n",
      "3 rewards 0.6931715368985589\n",
      "clear session data 49 2895568896\n",
      "4 rewards 0.6665282695648649\n",
      "clear session data 49 2895568896\n",
      "0 rewards 0.5394236987592367\n",
      "clear session data 49 2895568896\n",
      "1 rewards 0.7130368793608924\n",
      "clear session data 49 2895568896\n",
      "2 rewards 0.7249229435120979\n",
      "clear session data 49 2895568896\n",
      "3 rewards 0.855119523552428\n",
      "clear session data 49 2895568896\n",
      "4 rewards 0.750959802532293\n",
      "clear session data 49 2914672640\n",
      "0 rewards 0.4625906612549172\n",
      "clear session data 49 2914672640\n",
      "1 rewards 0.549820290373759\n",
      "clear session data 49 2914672640\n",
      "2 rewards 0.6621238085852057\n",
      "clear session data 49 2914672640\n",
      "3 rewards 0.6732224608663753\n",
      "clear session data 49 2914672640\n",
      "4 rewards 0.6920949678805004\n",
      "clear session data 49 2914852864\n",
      "0 rewards 0.3817069956053055\n",
      "clear session data 49 2914852864\n",
      "1 rewards 0.6665481932988024\n",
      "clear session data 49 2914852864\n",
      "2 rewards 0.7187768393605576\n",
      "clear session data 49 2914852864\n",
      "3 rewards 0.5894684691440806\n",
      "clear session data 49 2914852864\n",
      "4 rewards 0.6847079613095238\n"
     ]
    }
   ],
   "source": [
    "# punish on the border , changed action is fedback, dynamical alpha \n",
    "for iters, noise in enumerate(10 * [0.0]):\n",
    "    for trial in [39]: \n",
    "        Pretest =  PretrainTest(holes = 0, weight_write = 'weights_cpu1/rnn_1515tanh512_checkpoint{}'.format(trial))\n",
    "        weight_read = Pretest.weight\n",
    "        weight_write = 'weights_fix/weights1/rnn_1515tanh512_checkpoint{}_{}'.format(trial, iters+24)\n",
    "        rewards = Pretest.qlearn(weight_read,  weight_write, iterations = 5, noise = noise, size_train =[15], size_test=[15])\n",
    "        np.save('Rewards_{}_{}.npy'.format(iters+24, trial), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "759px",
    "left": "0px",
    "right": "1228px",
    "top": "67px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
